apiVersion: v1
items:
- apiVersion: alerting.kubesphere.io/v2beta1
  kind: GlobalRuleGroup
  metadata:
    annotations:
      alerting.kubesphere.io/initial-configuration: '{"apiVersion":"alerting.kubesphere.io/v2beta1","kind":"GlobalRuleGroup","metadata":{"annotations":{},"labels":{"alerting.kubesphere.io/builtin":"true","alerting.kubesphere.io/enable":"true"},"name":"thanos-rule","namespace":"kubesphere-monitoring-system"},"spec":{"rules":[{"alert":"ThanosRuleQueueIsDroppingAlerts","annotations":{"description":"Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is failing to queue alerts.","runbook_url":"https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulequeueisdroppingalerts","summary":"Thanos Rule is failing to queue alerts."},"expr":"sum by (cluster, namespace, job, instance) (rate(thanos_alert_queue_alerts_dropped_total{job=\"thanos-ruler-kubesphere\",namespace=\"kubesphere-monitoring-system\"}[5m])) > 0\n","for":"5m","labels":{},"severity":"critical"},{"alert":"ThanosRuleSenderIsFailingAlerts","annotations":{"description":"Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is failing to send alerts to alertmanager.","runbook_url":"https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulesenderisfailingalerts","summary":"Thanos Rule is failing to send alerts to alertmanager."},"expr":"sum by (cluster, namespace, job, instance) (rate(thanos_alert_sender_alerts_dropped_total{job=\"thanos-ruler-kubesphere\",namespace=\"kubesphere-monitoring-system\"}[5m])) > 0\n","for":"5m","labels":{},"severity":"critical"},{"alert":"ThanosRuleHighRuleEvaluationFailures","annotations":{"description":"Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is failing to evaluate rules.","runbook_url":"https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulehighruleevaluationfailures","summary":"Thanos Rule is failing to evaluate rules."},"expr":"(\n  sum by (cluster, namespace, job, instance) (rate(prometheus_rule_evaluation_failures_total{job=\"thanos-ruler-kubesphere\",namespace=\"kubesphere-monitoring-system\"}[5m]))\n/\n  sum by (cluster, namespace, job, instance) (rate(prometheus_rule_evaluations_total{job=\"thanos-ruler-kubesphere\",namespace=\"kubesphere-monitoring-system\"}[5m]))\n* 100 > 5\n)\n","for":"5m","labels":{},"severity":"critical"},{"alert":"ThanosRuleHighRuleEvaluationWarnings","annotations":{"description":"Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has high number of evaluation warnings.","runbook_url":"https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulehighruleevaluationwarnings","summary":"Thanos Rule has high number of evaluation warnings."},"expr":"sum by (cluster, namespace, job, instance) (rate(thanos_rule_evaluation_with_warnings_total{job=\"thanos-ruler-kubesphere\",namespace=\"kubesphere-monitoring-system\"}[5m])) > 0\n","for":"15m","labels":{},"severity":"info"},{"alert":"ThanosRuleRuleEvaluationLatencyHigh","annotations":{"description":"Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has higher evaluation latency than interval for {{$labels.rule_group}}.","runbook_url":"https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosruleruleevaluationlatencyhigh","summary":"Thanos Rule has high rule evaluation latency."},"expr":"(\n  sum by (cluster, namespace, job, instance, rule_group) (prometheus_rule_group_last_duration_seconds{job=\"thanos-ruler-kubesphere\",namespace=\"kubesphere-monitoring-system\"})\n>\n  sum by (cluster, namespace, job, instance, rule_group) (prometheus_rule_group_interval_seconds{job=\"thanos-ruler-kubesphere\",namespace=\"kubesphere-monitoring-system\"})\n)\n","for":"5m","labels":{},"severity":"warning"},{"alert":"ThanosRuleGrpcErrorRate","annotations":{"description":"Thanos Rule {{$labels.job}} in {{$labels.namespace}} is failing to handle {{$value | humanize}}% of requests.","runbook_url":"https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulegrpcerrorrate","summary":"Thanos Rule is failing to handle grpc requests."},"expr":"(\n  sum by (cluster, namespace, job, instance) (rate(grpc_server_handled_total{grpc_code=~\"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded\", job=\"thanos-ruler-kubesphere\",namespace=\"kubesphere-monitoring-system\"}[5m]))\n/\n  sum by (cluster, namespace, job, instance) (rate(grpc_server_started_total{job=\"thanos-ruler-kubesphere\",namespace=\"kubesphere-monitoring-system\"}[5m]))\n* 100 > 5\n)\n","for":"5m","labels":{},"severity":"warning"},{"alert":"ThanosRuleConfigReloadFailure","annotations":{"description":"Thanos Rule {{$labels.job}} in {{$labels.namespace}} has not been able to reload its configuration.","runbook_url":"https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosruleconfigreloadfailure","summary":"Thanos Rule has not been able to reload configuration."},"expr":"avg by (cluster, namespace, job, instance) (thanos_rule_config_last_reload_successful{job=\"thanos-ruler-kubesphere\",namespace=\"kubesphere-monitoring-system\"}) != 1","for":"5m","labels":{},"severity":"info"},{"alert":"ThanosRuleQueryHighDNSFailures","annotations":{"description":"Thanos Rule {{$labels.job}} in {{$labels.namespace}} has {{$value | humanize}}% of failing DNS queries for query endpoints.","runbook_url":"https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulequeryhighdnsfailures","summary":"Thanos Rule is having high number of DNS failures."},"expr":"(\n  sum by (cluster, namespace, job, instance) (rate(thanos_rule_query_apis_dns_failures_total{job=\"thanos-ruler-kubesphere\",namespace=\"kubesphere-monitoring-system\"}[5m]))\n/\n  sum by (cluster, namespace, job, instance) (rate(thanos_rule_query_apis_dns_lookups_total{job=\"thanos-ruler-kubesphere\",namespace=\"kubesphere-monitoring-system\"}[5m]))\n* 100 > 1\n)\n","for":"15m","labels":{},"severity":"warning"},{"alert":"ThanosRuleAlertmanagerHighDNSFailures","annotations":{"description":"Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has {{$value | humanize}}% of failing DNS queries for Alertmanager endpoints.","runbook_url":"https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulealertmanagerhighdnsfailures","summary":"Thanos Rule is having high number of DNS failures."},"expr":"(\n  sum by (cluster, namespace, job, instance) (rate(thanos_rule_alertmanagers_dns_failures_total{job=\"thanos-ruler-kubesphere\",namespace=\"kubesphere-monitoring-system\"}[5m]))\n/\n  sum by (cluster, namespace, job, instance) (rate(thanos_rule_alertmanagers_dns_lookups_total{job=\"thanos-ruler-kubesphere\",namespace=\"kubesphere-monitoring-system\"}[5m]))\n* 100 > 1\n)\n","for":"15m","labels":{},"severity":"warning"},{"alert":"ThanosRuleNoEvaluationFor10Intervals","annotations":{"description":"Thanos Rule {{$labels.job}} in {{$labels.namespace}} has {{$value | humanize}}% rule groups that did not evaluate for at least 10x of their expected interval.","runbook_url":"https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulenoevaluationfor10intervals","summary":"Thanos Rule has rule groups that did not evaluate for 10 intervals."},"expr":"time() -  max by (cluster, namespace, job, instance, group) (prometheus_rule_group_last_evaluation_timestamp_seconds{job=\"thanos-ruler-kubesphere\",namespace=\"kubesphere-monitoring-system\"})\n>\n10 * max by (cluster, namespace, job, instance, group) (prometheus_rule_group_interval_seconds{job=\"thanos-ruler-kubesphere\",namespace=\"kubesphere-monitoring-system\"})\n","for":"5m","labels":{},"severity":"info"},{"alert":"ThanosNoRuleEvaluations","annotations":{"description":"Thanos Rule {{$labels.instance}} in {{$labels.namespace}} did not perform any rule evaluations in the past 10 minutes.","runbook_url":"https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosnoruleevaluations","summary":"Thanos Rule did not perform any rule evaluations."},"expr":"sum by (cluster, namespace, job, instance) (rate(prometheus_rule_evaluations_total{job=\"thanos-ruler-kubesphere\",namespace=\"kubesphere-monitoring-system\"}[5m])) <= 0\n  and\nsum by (cluster, namespace, job, instance) (thanos_rule_loaded_rules{job=\"thanos-ruler-kubesphere\",namespace=\"kubesphere-monitoring-system\"}) > 0\n","for":"5m","labels":{},"severity":"critical"}]}}'
    labels:
      alerting.kubesphere.io/builtin: "true"
      alerting.kubesphere.io/enable: "true"
    name: thanos-rule
    namespace: kubesphere-monitoring-system
  spec:
    rules:
    - alert: ThanosRuleQueueIsDroppingAlerts
      annotations:
        description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is failing to queue alerts.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulequeueisdroppingalerts
        summary: Thanos Rule is failing to queue alerts.
      expr: |
        sum by (cluster, namespace, job, instance) (rate(thanos_alert_queue_alerts_dropped_total{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m])) > 0
      for: 5m
      labels: {}
      severity: critical
    - alert: ThanosRuleSenderIsFailingAlerts
      annotations:
        description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is failing to send alerts to alertmanager.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulesenderisfailingalerts
        summary: Thanos Rule is failing to send alerts to alertmanager.
      expr: |
        sum by (cluster, namespace, job, instance) (rate(thanos_alert_sender_alerts_dropped_total{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m])) > 0
      for: 5m
      labels: {}
      severity: critical
    - alert: ThanosRuleHighRuleEvaluationFailures
      annotations:
        description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is failing to evaluate rules.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulehighruleevaluationfailures
        summary: Thanos Rule is failing to evaluate rules.
      expr: |
        (
          sum by (cluster, namespace, job, instance) (rate(prometheus_rule_evaluation_failures_total{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m]))
        /
          sum by (cluster, namespace, job, instance) (rate(prometheus_rule_evaluations_total{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m]))
        * 100 > 5
        )
      for: 5m
      labels: {}
      severity: critical
    - alert: ThanosRuleHighRuleEvaluationWarnings
      annotations:
        description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has high number of evaluation warnings.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulehighruleevaluationwarnings
        summary: Thanos Rule has high number of evaluation warnings.
      expr: |
        sum by (cluster, namespace, job, instance) (rate(thanos_rule_evaluation_with_warnings_total{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m])) > 0
      for: 15m
      labels: {}
      severity: info
    - alert: ThanosRuleRuleEvaluationLatencyHigh
      annotations:
        description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has higher evaluation latency than interval for {{$labels.rule_group}}.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosruleruleevaluationlatencyhigh
        summary: Thanos Rule has high rule evaluation latency.
      expr: |
        (
          sum by (cluster, namespace, job, instance, rule_group) (prometheus_rule_group_last_duration_seconds{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"})
        >
          sum by (cluster, namespace, job, instance, rule_group) (prometheus_rule_group_interval_seconds{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"})
        )
      for: 5m
      labels: {}
      severity: warning
    - alert: ThanosRuleGrpcErrorRate
      annotations:
        description: Thanos Rule {{$labels.job}} in {{$labels.namespace}} is failing to handle {{$value | humanize}}% of requests.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulegrpcerrorrate
        summary: Thanos Rule is failing to handle grpc requests.
      expr: |
        (
          sum by (cluster, namespace, job, instance) (rate(grpc_server_handled_total{grpc_code=~"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded", job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m]))
        /
          sum by (cluster, namespace, job, instance) (rate(grpc_server_started_total{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m]))
        * 100 > 5
        )
      for: 5m
      labels: {}
      severity: warning
    - alert: ThanosRuleConfigReloadFailure
      annotations:
        description: Thanos Rule {{$labels.job}} in {{$labels.namespace}} has not been able to reload its configuration.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosruleconfigreloadfailure
        summary: Thanos Rule has not been able to reload configuration.
      expr: avg by (cluster, namespace, job, instance) (thanos_rule_config_last_reload_successful{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}) != 1
      for: 5m
      labels: {}
      severity: info
    - alert: ThanosRuleQueryHighDNSFailures
      annotations:
        description: Thanos Rule {{$labels.job}} in {{$labels.namespace}} has {{$value | humanize}}% of failing DNS queries for query endpoints.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulequeryhighdnsfailures
        summary: Thanos Rule is having high number of DNS failures.
      expr: |
        (
          sum by (cluster, namespace, job, instance) (rate(thanos_rule_query_apis_dns_failures_total{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m]))
        /
          sum by (cluster, namespace, job, instance) (rate(thanos_rule_query_apis_dns_lookups_total{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m]))
        * 100 > 1
        )
      for: 15m
      labels: {}
      severity: warning
    - alert: ThanosRuleAlertmanagerHighDNSFailures
      annotations:
        description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has {{$value | humanize}}% of failing DNS queries for Alertmanager endpoints.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulealertmanagerhighdnsfailures
        summary: Thanos Rule is having high number of DNS failures.
      expr: |
        (
          sum by (cluster, namespace, job, instance) (rate(thanos_rule_alertmanagers_dns_failures_total{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m]))
        /
          sum by (cluster, namespace, job, instance) (rate(thanos_rule_alertmanagers_dns_lookups_total{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m]))
        * 100 > 1
        )
      for: 15m
      labels: {}
      severity: warning
    - alert: ThanosRuleNoEvaluationFor10Intervals
      annotations:
        description: Thanos Rule {{$labels.job}} in {{$labels.namespace}} has {{$value | humanize}}% rule groups that did not evaluate for at least 10x of their expected interval.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulenoevaluationfor10intervals
        summary: Thanos Rule has rule groups that did not evaluate for 10 intervals.
      expr: |
        time() -  max by (cluster, namespace, job, instance, group) (prometheus_rule_group_last_evaluation_timestamp_seconds{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"})
        >
        10 * max by (cluster, namespace, job, instance, group) (prometheus_rule_group_interval_seconds{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"})
      for: 5m
      labels: {}
      severity: info
    - alert: ThanosNoRuleEvaluations
      annotations:
        description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} did not perform any rule evaluations in the past 10 minutes.
        runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosnoruleevaluations
        summary: Thanos Rule did not perform any rule evaluations.
      expr: |
        sum by (cluster, namespace, job, instance) (rate(prometheus_rule_evaluations_total{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m])) <= 0
          and
        sum by (cluster, namespace, job, instance) (thanos_rule_loaded_rules{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}) > 0
      for: 5m
      labels: {}
      severity: critical
kind: List
