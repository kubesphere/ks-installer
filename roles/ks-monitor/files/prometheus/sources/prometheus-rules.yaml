apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: k8s
    role: alert-rules
  name: prometheus-k8s-rules
  namespace: kubesphere-monitoring-system
spec:
  groups:
    - name: k8s.rules
      rules:
        - expr: |
            sum((container_cpu_usage_seconds_total{job="kubelet", image!="", container_name!=""} * on(namespace) group_left(label_kubesphere_io_workspace) kube_namespace_labels{job="kube-state-metrics"} - container_cpu_usage_seconds_total{job="kubelet", image!="", container_name!=""} offset 90s * on(namespace) group_left(label_kubesphere_io_workspace) kube_namespace_labels{job="kube-state-metrics"}) / 90) by (namespace, label_kubesphere_io_workspace)
          record: namespace:container_cpu_usage_seconds_total:sum_rate
        - expr: |
            count((container_accelerator_memory_used_bytes {job="cadvsior"}) * on(namespace) group_left(label_kubesphere_io_workspace) kube_namespace_labels{job="kube-state-metrics"}) by (namespace, label_kubesphere_io_workspace)
          record: namespace:container_gpu_usage:count
        - expr: |
            sum(container_memory_usage_bytes{job="kubelet", image!="", container_name!=""} * on(namespace) group_left(label_kubesphere_io_workspace) kube_namespace_labels{job="kube-state-metrics"}) by (namespace, label_kubesphere_io_workspace)
          record: namespace:container_memory_usage_bytes:sum
        - expr: |
            sum((container_memory_usage_bytes{job="kubelet", image!="", container_name!=""} - container_memory_cache{job="kubelet", image!="", container_name!=""}) * on(namespace) group_left(label_kubesphere_io_workspace) kube_namespace_labels{job="kube-state-metrics"}) by (namespace, label_kubesphere_io_workspace)
          record: namespace:container_memory_usage_bytes_wo_cache:sum
    - name: node.rules
      rules:
        - expr: |
            max(label_replace(kube_pod_info{job="kube-state-metrics"}, "pod", "$1", "pod", "(.*)")) by (node, namespace, pod)
          record: 'node_namespace_pod:kube_pod_info:'
        - expr: |
            count by (node) (sum by (node, cpu) (
              node_cpu_seconds_total{job="node-exporter"}
            * on (namespace, pod) group_left(node)
              node_namespace_pod:kube_pod_info:
            ))
          record: node:node_num_cpu:sum
        - expr: |
            avg(irate(node_cpu_seconds_total{job="node-exporter",mode="used"}[5m]))
          record: :node_cpu_utilisation:avg1m
        - expr: |
            avg by (node) (
              irate(node_cpu_seconds_total{job="node-exporter",mode="used"}[5m])
            * on (namespace, pod) group_left(node)
              node_namespace_pod:kube_pod_info:)
          record: node:node_cpu_utilisation:avg1m
        - expr: |
            1 -
            sum(node_memory_MemFree_bytes{job="node-exporter"} + node_memory_Cached_bytes{job="node-exporter"} + node_memory_Buffers_bytes{job="node-exporter"} + node_memory_SReclaimable_bytes{job="node-exporter"})
            /
            sum(node_memory_MemTotal_bytes{job="node-exporter"})
          record: ':node_memory_utilisation:'
        - expr: |
            sum by (node) (
              (node_memory_MemFree_bytes{job="node-exporter"} + node_memory_Cached_bytes{job="node-exporter"} + node_memory_Buffers_bytes{job="node-exporter"} + node_memory_SReclaimable_bytes{job="node-exporter"})
              * on (namespace, pod) group_left(node)
                node_namespace_pod:kube_pod_info:
            )
          record: node:node_memory_bytes_available:sum
        - expr: |
            sum by (node) (
              node_memory_MemTotal_bytes{job="node-exporter"}
              * on (namespace, pod) group_left(node)
                node_namespace_pod:kube_pod_info:
            )
          record: node:node_memory_bytes_total:sum
        - expr: |
            1 - (node:node_memory_bytes_available:sum / node:node_memory_bytes_total:sum)
          record: 'node:node_memory_utilisation:'
        - expr: |
            sum by (node) (
              irate(node_disk_reads_completed_total{job="node-exporter"}[5m])
            * on (namespace, pod) group_left(node)
              node_namespace_pod:kube_pod_info:
            )
          record: node:data_volume_iops_reads:sum
        - expr: |
            sum by (node) (
              irate(node_disk_writes_completed_total{job="node-exporter"}[5m])
            * on (namespace, pod) group_left(node)
              node_namespace_pod:kube_pod_info:
            )
          record: node:data_volume_iops_writes:sum
        - expr: |
            sum by (node) (
              irate(node_disk_read_bytes_total{job="node-exporter"}[5m])
            * on (namespace, pod) group_left(node)
              node_namespace_pod:kube_pod_info:
            )
          record: node:data_volume_throughput_bytes_read:sum
        - expr: |
            sum by (node) (
              irate(node_disk_written_bytes_total{job="node-exporter"}[5m])
            * on (namespace, pod) group_left(node)
              node_namespace_pod:kube_pod_info:
            )
          record: node:data_volume_throughput_bytes_written:sum
        - expr: |
            sum(irate(node_network_receive_bytes_total{job="node-exporter",device!~"^(cali.+|tunl.+|dummy.+|kube.+|flannel.+|cni.+|docker.+|veth.+|lo.*)"}[5m])) +
            sum(irate(node_network_transmit_bytes_total{job="node-exporter",device!~"^(cali.+|tunl.+|dummy.+|kube.+|flannel.+|cni.+|docker.+|veth.+|lo.*)"}[5m]))
          record: :node_net_utilisation:sum_irate
        - expr: |
            sum by (node) (
              (irate(node_network_receive_bytes_total{job="node-exporter",device!~"^(cali.+|tunl.+|dummy.+|kube.+|flannel.+|cni.+|docker.+|veth.+|lo.*)"}[5m]) +
              irate(node_network_transmit_bytes_total{job="node-exporter",device!~"^(cali.+|tunl.+|dummy.+|kube.+|flannel.+|cni.+|docker.+|veth.+|lo.*)"}[5m]))
            * on (namespace, pod) group_left(node)
              node_namespace_pod:kube_pod_info:
            )
          record: node:node_net_utilisation:sum_irate
        - expr: |
            sum by (node) (
              irate(node_network_transmit_bytes_total{job="node-exporter",device!~"^(cali.+|tunl.+|dummy.+|kube.+|flannel.+|cni.+|docker.+|veth.+|lo.*)"}[5m])
            * on (namespace, pod) group_left(node)
              node_namespace_pod:kube_pod_info:
            )
          record: node:node_net_bytes_transmitted:sum_irate
        - expr: |
            sum by (node) (
              irate(node_network_receive_bytes_total{job="node-exporter",device!~"^(cali.+|tunl.+|dummy.+|kube.+|flannel.+|cni.+|docker.+|veth.+|lo.*)"}[5m])
            * on (namespace, pod) group_left(node)
              node_namespace_pod:kube_pod_info:
            )
          record: node:node_net_bytes_received:sum_irate
        - expr: |
            sum by(node) (sum(max(node_filesystem_files{device=~"/dev/.*", device!~"/dev/loop\\d+", job="node-exporter"}) by (device, pod, namespace)) by (pod, namespace) * on (namespace, pod) group_left(node) node_namespace_pod:kube_pod_info:)
          record: 'node:node_inodes_total:'
        - expr: |
            sum by(node) (sum(max(node_filesystem_files_free{device=~"/dev/.*", device!~"/dev/loop\\d+", job="node-exporter"}) by (device, pod, namespace)) by (pod, namespace) * on (namespace, pod) group_left(node) node_namespace_pod:kube_pod_info:)
          record: 'node:node_inodes_free:'
        - expr: |
            sum by (node) (node_load1{job="node-exporter"} * on (namespace, pod) group_left(node) node_namespace_pod:kube_pod_info:) / node:node_num_cpu:sum
          record: node:load1:ratio
        - expr: |
            sum by (node) (node_load5{job="node-exporter"} * on (namespace, pod) group_left(node) node_namespace_pod:kube_pod_info:) / node:node_num_cpu:sum
          record: node:load5:ratio
        - expr: |
            sum by (node) (node_load15{job="node-exporter"} * on (namespace, pod) group_left(node) node_namespace_pod:kube_pod_info:) / node:node_num_cpu:sum
          record: node:load15:ratio
        - expr: |
            sum by (node) ((kube_pod_status_scheduled{job="kube-state-metrics", condition="true"} > 0)  * on (namespace, pod) group_left(node) kube_pod_info{job="kube-state-metrics"} unless on (node) (kube_node_status_condition{job="kube-state-metrics", condition="Ready", status=~"unknown|false"} > 0))
          record: node:pod_count:sum
        - expr: |
            (node:pod_running:count / sum(kube_node_status_capacity_pods) by (node)) unless on (node) (kube_node_status_condition{job="kube-state-metrics", condition="Ready", status=~"unknown|false"} > 0)
          record: node:pod_utilization:ratio
        - expr: |
            count(kube_pod_info{job="kube-state-metrics"} unless on (pod, namespace) (kube_pod_status_phase{job="kube-state-metrics", phase=~"Failed|Pending|Unknown|Succeeded"} > 0))  by (node) unless on (node) (kube_node_status_condition{job="kube-state-metrics", condition="Ready", status=~"unknown|false"} > 0)
          record: node:pod_running:count
        - expr: |
            count(kube_pod_info{job="kube-state-metrics"} unless on (pod, namespace) (kube_pod_status_phase{job="kube-state-metrics", phase=~"Failed|Pending|Unknown|Running"} > 0))  by (node) unless on (node) (kube_node_status_condition{job="kube-state-metrics", condition="Ready",status=~"unknown|false"} > 0)
          record: node:pod_succeeded:count
        - expr: |
            count(kube_pod_info{job="kube-state-metrics", node!=""} unless on (pod, namespace) (kube_pod_status_phase{job="kube-state-metrics", phase="Succeeded"}>0) unless on (pod, namespace) ((kube_pod_status_ready{job="kube-state-metrics", condition="true"}>0) and on (pod, namespace) (kube_pod_status_phase{job="kube-state-metrics", phase="Running"}>0)) unless on (pod, namespace) kube_pod_container_status_waiting_reason{job="kube-state-metrics", reason="ContainerCreating"}>0) by (node)
          record: node:pod_abnormal:count
        - expr: |
            (node:pod_abnormal:count / count(kube_pod_info{job="kube-state-metrics", node!=""} unless on (pod, namespace) kube_pod_status_phase{job="kube-state-metrics", phase="Succeeded"}>0) by (node)) unless on (node) (kube_node_status_condition{job="kube-state-metrics", condition="Ready", status=~"unknown|false"} > 0)
          record: node:pod_abnormal:ratio
        - expr: |
            sum(max(node_filesystem_avail_bytes{device=~"/dev/.*", device!~"/dev/loop\\d+", job="node-exporter"} * on (namespace, pod) group_left(node) node_namespace_pod:kube_pod_info:) by (device, node)) by (node)
          record: 'node:disk_space_available:'
        - expr: |
            1- sum(max(node_filesystem_avail_bytes{device=~"/dev/.*", device!~"/dev/loop\\d+", job="node-exporter"} * on (namespace, pod) group_left(node) node_namespace_pod:kube_pod_info:) by (device, node)) by (node) / sum(max(node_filesystem_size_bytes{device=~"/dev/.*", device!~"/dev/loop\\d+", job="node-exporter"} * on (namespace, pod) group_left(node) node_namespace_pod:kube_pod_info:) by (device, node)) by (node)
          record: node:disk_space_utilization:ratio
        - expr: |
            (1 - (node:node_inodes_free: / node:node_inodes_total:))
          record: node:disk_inode_utilization:ratio
    - name: cluster.rules
      rules:
        - expr: |
            count(kube_pod_info{job="kube-state-metrics"} unless on (pod, namespace) (kube_pod_status_phase{job="kube-state-metrics", phase="Succeeded"}>0) unless on (pod, namespace) ((kube_pod_status_ready{job="kube-state-metrics", condition="true"}>0) and on (pod, namespace) (kube_pod_status_phase{job="kube-state-metrics", phase="Running"}>0)) unless on (pod, namespace) kube_pod_container_status_waiting_reason{job="kube-state-metrics", reason="ContainerCreating"}>0)
          record: cluster:pod_abnormal:sum
        - expr: |
            sum((kube_pod_status_scheduled{job="kube-state-metrics", condition="true"} > 0)  * on (namespace, pod) group_left(node) (sum by (node, namespace, pod) (kube_pod_info)) unless on (node) (kube_node_status_condition{job="kube-state-metrics", condition="Ready",status=~"unknown|false"} > 0))
          record: cluster:pod:sum
        - expr: |
            cluster:pod_abnormal:sum / sum(kube_pod_status_phase{job="kube-state-metrics", phase!="Succeeded"})
          record: cluster:pod_abnormal:ratio
        - expr: |
            count(kube_pod_info{job="kube-state-metrics"} and on (pod, namespace) (kube_pod_status_phase{job="kube-state-metrics", phase="Running"}>0) unless on (node) (kube_node_status_condition{job="kube-state-metrics", condition="Ready", status=~"unknown|false"} > 0))
          record: cluster:pod_running:count
        - expr: |
            cluster:pod_running:count / sum(kube_node_status_capacity_pods unless on (node) (kube_node_status_condition{job="kube-state-metrics", condition="Ready", status=~"unknown|false"} > 0))
          record: cluster:pod_utilization:ratio
        - expr: |
            1 - sum(max(node_filesystem_avail_bytes{device=~"/dev/.*", device!~"/dev/loop\\d+", job="node-exporter"}) by (device, instance)) / sum(max(node_filesystem_size_bytes{device=~"/dev/.*", device!~"/dev/loop\\d+", job="node-exporter"}) by (device, instance))
          record: cluster:disk_utilization:ratio
        - expr: |
            1 - sum(node:node_inodes_free:) / sum(node:node_inodes_total:)
          record: cluster:disk_inode_utilization:ratio
        - expr: |
            sum(kube_node_status_condition{job="kube-state-metrics", condition="Ready", status=~"unknown|false"})
          record: cluster:node_offline:sum
        - expr: |
            sum(kube_node_status_condition{job="kube-state-metrics", condition="Ready", status=~"unknown|false"}) / sum(kube_node_status_condition{job="kube-state-metrics", condition="Ready"})
          record: cluster:node_offline:ratio
    - name: namespace.rules
      rules:
        - expr: |
            (count(kube_pod_info{job="kube-state-metrics", node!=""}) by (namespace) - sum(kube_pod_status_phase{job="kube-state-metrics", phase="Succeeded"}) by (namespace)  - sum(kube_pod_status_ready{job="kube-state-metrics", condition="true"} * on (pod, namespace) kube_pod_status_phase{job="kube-state-metrics", phase="Running"}) by (namespace) - sum(kube_pod_container_status_waiting_reason{job="kube-state-metrics", reason="ContainerCreating"}) by (namespace)) * on (namespace) group_left(label_kubesphere_io_workspace)(kube_namespace_labels{job="kube-state-metrics"})
          record: namespace:pod_abnormal:count
        - expr: |
            namespace:pod_abnormal:count / (sum(kube_pod_status_phase{job="kube-state-metrics", phase!="Succeeded", namespace!=""}) by (namespace) * on (namespace) group_left(label_kubesphere_io_workspace)(kube_namespace_labels{job="kube-state-metrics"}))
          record: namespace:pod_abnormal:ratio
        - expr: |
            max(kube_resourcequota{job="kube-state-metrics", type="used"}) by (resource, namespace) / min(kube_resourcequota{job="kube-state-metrics", type="hard"}) by (resource, namespace) *  on (namespace) group_left(label_kubesphere_io_workspace) (kube_namespace_labels{job="kube-state-metrics"})
          record: namespace:resourcequota_used:ratio
        - expr: |
            sum (label_replace(label_join(sum(irate(container_cpu_usage_seconds_total{job="kubelet", pod_name!="", image!=""}[5m])) by (namespace, pod_name) * on (pod_name, namespace) group_left(owner_kind,owner_name) label_join(label_replace(label_join(label_replace(label_replace(kube_pod_owner{job="kube-state-metrics"},"owner_kind", "Deployment", "owner_kind", "ReplicaSet"), "owner_kind", "Pod", "owner_kind", "<none>"),"tmp",":","owner_name","pod"),"owner_name","$1","tmp","<none>:(.*)"), "pod_name", "", "pod", "_name"), "workload",":","owner_kind","owner_name"), "workload","$1","workload","(Deployment:.+)-(.+)")) by (namespace, workload, owner_kind)
          record: namespace:workload_cpu_usage:sum
        - expr: |
            sum (label_replace(label_join(sum(container_memory_usage_bytes{job="kubelet", pod_name!="", image!=""}) by (namespace, pod_name) * on (pod_name, namespace) group_left(owner_kind,owner_name) label_join(label_replace(label_join(label_replace(label_replace(kube_pod_owner{job="kube-state-metrics"},"owner_kind", "Deployment", "owner_kind", "ReplicaSet"), "owner_kind", "Pod", "owner_kind", "<none>"),"tmp",":","owner_name","pod"),"owner_name","$1","tmp","<none>:(.*)"), "pod_name", "", "pod", "_name"), "workload",":","owner_kind","owner_name"), "workload","$1","workload","(Deployment:.+)-(.+)")) by (namespace, workload, owner_kind)
          record: namespace:workload_memory_usage:sum
        - expr: |
            sum (label_replace(label_join(sum(container_memory_usage_bytes{job="kubelet", pod_name!="", image!=""} - container_memory_cache{job="kubelet", pod_name!="", image!=""}) by (namespace, pod_name) * on (pod_name, namespace) group_left(owner_kind,owner_name) label_join(label_replace(label_join(label_replace(label_replace(kube_pod_owner{job="kube-state-metrics"},"owner_kind", "Deployment", "owner_kind", "ReplicaSet"), "owner_kind", "Pod", "owner_kind", "<none>"),"tmp",":","owner_name","pod"),"owner_name","$1","tmp","<none>:(.*)"), "pod_name", "", "pod", "_name"), "workload",":","owner_kind","owner_name"), "workload","$1","workload","(Deployment:.+)-(.+)")) by (namespace, workload, owner_kind)
          record: namespace:workload_memory_usage_wo_cache:sum
        - expr: |
            sum (label_replace(label_join(sum(irate(container_network_transmit_bytes_total{pod_name!="", interface!~"^(cali.+|tunl.+|dummy.+|kube.+|flannel.+|cni.+|docker.+|veth.+|lo.*)", job="kubelet"}[5m])) by (namespace, pod_name) * on (pod_name, namespace) group_left(owner_kind,owner_name) label_join(label_replace(label_join(label_replace(label_replace(kube_pod_owner{job="kube-state-metrics"},"owner_kind", "Deployment", "owner_kind", "ReplicaSet"), "owner_kind", "Pod", "owner_kind", "<none>"),"tmp",":","owner_name","pod"),"owner_name","$1","tmp","<none>:(.*)"), "pod_name", "", "pod", "_name"), "workload",":","owner_kind","owner_name"), "workload","$1","workload","(Deployment:.+)-(.+)")) by (namespace, workload, owner_kind)
          record: namespace:workload_net_bytes_transmitted:sum_irate
        - expr: |
            sum (label_replace(label_join(sum(irate(container_network_receive_bytes_total{pod_name!="", interface!~"^(cali.+|tunl.+|dummy.+|kube.+|flannel.+|cni.+|docker.+|veth.+|lo.*)", job="kubelet"}[5m])) by (namespace, pod_name) * on (pod_name, namespace) group_left(owner_kind,owner_name) label_join(label_replace(label_join(label_replace(label_replace(kube_pod_owner{job="kube-state-metrics"},"owner_kind", "Deployment", "owner_kind", "ReplicaSet"), "owner_kind", "Pod", "owner_kind", "<none>"),"tmp",":","owner_name","pod"),"owner_name","$1","tmp","<none>:(.*)"), "pod_name", "", "pod", "_name"), "workload",":","owner_kind","owner_name"), "workload","$1","workload","(Deployment:.+)-(.+)")) by (namespace, workload, owner_kind)
          record: namespace:workload_net_bytes_received:sum_irate
        - expr: |
            label_replace(label_replace(sum(kube_deployment_status_replicas_unavailable{job="kube-state-metrics"}) by (deployment, namespace) / sum(kube_deployment_spec_replicas{job="kube-state-metrics"}) by (deployment, namespace) * on (namespace) group_left(label_kubesphere_io_workspace)(kube_namespace_labels{job="kube-state-metrics"}), "workload","Deployment:$1", "deployment", "(.*)"), "owner_kind","Deployment", "", "")
          record: namespace:deployment_unavailable_replicas:ratio
        - expr: |
            label_replace(label_replace(sum(kube_daemonset_status_number_unavailable{job="kube-state-metrics"}) by (daemonset, namespace) / sum(kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}) by (daemonset, namespace) * on (namespace) group_left(label_kubesphere_io_workspace)(kube_namespace_labels{job="kube-state-metrics"}) , "workload","DaemonSet:$1", "daemonset", "(.*)"), "owner_kind","DaemonSet", "", "")
          record: namespace:daemonset_unavailable_replicas:ratio
        - expr: |
            label_replace(label_replace((1 - sum(kube_statefulset_status_replicas_current{job="kube-state-metrics"}) by (statefulset, namespace) / sum(kube_statefulset_replicas{job="kube-state-metrics"}) by (statefulset, namespace)) * on (namespace) group_left(label_kubesphere_io_workspace)(kube_namespace_labels{job="kube-state-metrics"}) , "workload","StatefulSet:$1", "statefulset", "(.*)"), "owner_kind","StatefulSet", "", "")
          record: namespace:statefulset_unavailable_replicas:ratio
    - name: etcd.rules
      rules:
        - expr: |
            sum(up{job="etcd"} == 1)
          record: etcd:up:sum
        - expr: |
            sum(label_replace(sum(changes(etcd_server_leader_changes_seen_total{job="etcd"}[1h])) by (instance), "node", "$1", "instance", "(.*):.*")) by (node)
          record: etcd:etcd_server_leader_changes_seen:sum_changes
        - expr: |
            sum(label_replace(sum(irate(etcd_server_proposals_failed_total{job="etcd"}[5m])) by (instance), "node", "$1", "instance", "(.*):.*")) by (node)
          record: etcd:etcd_server_proposals_failed:sum_irate
        - expr: |
            sum(label_replace(sum(irate(etcd_server_proposals_applied_total{job="etcd"}[5m])) by (instance), "node", "$1", "instance", "(.*):.*")) by (node)
          record: etcd:etcd_server_proposals_applied:sum_irate
        - expr: |
            sum(label_replace(sum(irate(etcd_server_proposals_committed_total{job="etcd"}[5m])) by (instance), "node", "$1", "instance", "(.*):.*")) by (node)
          record: etcd:etcd_server_proposals_committed:sum_irate
        - expr: |
            sum(label_replace(sum(etcd_server_proposals_pending{job="etcd"}) by (instance), "node", "$1", "instance", "(.*):.*")) by (node)
          record: etcd:etcd_server_proposals_pending:sum
        - expr: |
            sum(label_replace(etcd_debugging_mvcc_db_total_size_in_bytes{job="etcd"},"node", "$1", "instance", "(.*):.*")) by (node)
          record: etcd:etcd_debugging_mvcc_db_total_size:sum
        - expr: |
            sum(label_replace(etcd_mvcc_db_total_size_in_bytes{job="etcd"},"node", "$1", "instance", "(.*):.*")) by (node)
          record: etcd:etcd_mvcc_db_total_size:sum
        - expr: |
            sum(label_replace(sum(irate(etcd_network_client_grpc_received_bytes_total{job="etcd"}[5m])) by (instance), "node", "$1", "instance", "(.*):.*")) by (node)
          record: etcd:etcd_network_client_grpc_received_bytes:sum_irate
        - expr: |
            sum(label_replace(sum(irate(etcd_network_client_grpc_sent_bytes_total{job="etcd"}[5m])) by (instance), "node", "$1", "instance", "(.*):.*")) by (node)
          record: etcd:etcd_network_client_grpc_sent_bytes:sum_irate
        - expr: |
            sum(label_replace(sum(irate(grpc_server_started_total{job="etcd",grpc_type="unary"}[5m])) by (instance), "node", "$1", "instance", "(.*):.*")) by (node)
          record: etcd:grpc_server_started:sum_irate
        - expr: |
            sum(label_replace(sum(irate(grpc_server_handled_total{job="etcd",grpc_type="unary",grpc_code!="OK"}[5m])) by (instance), "node", "$1", "instance", "(.*):.*")) by (node)
          record: etcd:grpc_server_handled:sum_irate
        - expr: |
            sum(label_replace(sum(irate(grpc_server_msg_received_total{job="etcd"}[5m])) by (instance), "node", "$1", "instance", "(.*):.*")) by (node)
          record: etcd:grpc_server_msg_received:sum_irate
        - expr: |
            sum(label_replace(sum(irate(grpc_server_msg_sent_total{job="etcd"}[5m])) by (instance), "node", "$1", "instance", "(.*):.*")) by (node)
          record: etcd:grpc_server_msg_sent:sum_irate
        - expr: |
            sum(label_replace(sum(irate(etcd_disk_wal_fsync_duration_seconds_sum{job="etcd"}[5m])) by (instance) / sum(irate(etcd_disk_wal_fsync_duration_seconds_count{job="etcd"}[5m])) by (instance), "node", "$1", "instance", "(.*):.*")) by (node)
          record: etcd:etcd_disk_wal_fsync_duration:avg
        - expr: |
            sum(label_replace(sum(irate(etcd_disk_backend_commit_duration_seconds_sum{job="etcd"}[5m])) by (instance) / sum(irate(etcd_disk_backend_commit_duration_seconds_count{job="etcd"}[5m])) by (instance), "node", "$1", "instance", "(.*):.*")) by (node)
          record: etcd:etcd_disk_backend_commit_duration:avg
    - name: etcd_histogram.rules
      rules:
        - expr: |
            histogram_quantile(0.99, sum(label_replace(sum(irate(etcd_disk_wal_fsync_duration_seconds_bucket{job="etcd"}[5m])) by (instance, le), "node", "$1", "instance", "(.*):.*")) by (node, le))
          labels:
            quantile: "0.99"
          record: etcd:etcd_disk_wal_fsync_duration:histogram_quantile
        - expr: |
            histogram_quantile(0.9, sum(label_replace(sum(irate(etcd_disk_wal_fsync_duration_seconds_bucket{job="etcd"}[5m])) by (instance, le), "node", "$1", "instance", "(.*):.*")) by (node, le))
          labels:
            quantile: "0.9"
          record: etcd:etcd_disk_wal_fsync_duration:histogram_quantile
        - expr: |
            histogram_quantile(0.5, sum(label_replace(sum(irate(etcd_disk_wal_fsync_duration_seconds_bucket{job="etcd"}[5m])) by (instance, le), "node", "$1", "instance", "(.*):.*")) by (node, le))
          labels:
            quantile: "0.5"
          record: etcd:etcd_disk_wal_fsync_duration:histogram_quantile
        - expr: |
            histogram_quantile(0.99, sum(label_replace(sum(irate(etcd_disk_backend_commit_duration_seconds_bucket{job="etcd"}[5m])) by (instance, le), "node", "$1", "instance", "(.*):.*")) by (node, le))
          labels:
            quantile: "0.99"
          record: etcd:etcd_disk_backend_commit_duration:histogram_quantile
        - expr: |
            histogram_quantile(0.9, sum(label_replace(sum(irate(etcd_disk_backend_commit_duration_seconds_bucket{job="etcd"}[5m])) by (instance, le), "node", "$1", "instance", "(.*):.*")) by (node, le))
          labels:
            quantile: "0.9"
          record: etcd:etcd_disk_backend_commit_duration:histogram_quantile
        - expr: |
            histogram_quantile(0.5, sum(label_replace(sum(irate(etcd_disk_backend_commit_duration_seconds_bucket{job="etcd"}[5m])) by (instance, le), "node", "$1", "instance", "(.*):.*")) by (node, le))
          labels:
            quantile: "0.5"
          record: etcd:etcd_disk_backend_commit_duration:histogram_quantile
    - name: apiserver.rules
      rules:
        - expr: |
            sum(up{job="apiserver"} == 1)
          record: apiserver:up:sum
        - expr: |
            sum(irate(apiserver_request_count{job="apiserver"}[5m]))
          record: apiserver:apiserver_request_count:sum_irate
        - expr: |
            sum(irate(apiserver_request_count{job="apiserver"}[5m])) by (verb)
          record: apiserver:apiserver_request_count:sum_verb_irate
        - expr: |
            sum(irate(apiserver_request_latencies_sum{job="apiserver", verb!~"WATCH|CONNECT"}[5m])) / sum(irate(apiserver_request_latencies_count{job="apiserver", verb!~"WATCH|CONNECT"}[5m]))/ 1e+06
          record: apiserver:apiserver_request_latencies:avg
        - expr: |
            sum(irate(apiserver_request_latencies_sum{job="apiserver", verb!~"WATCH|CONNECT"}[5m])) by (verb) / sum(irate(apiserver_request_latencies_count{job="apiserver", verb!~"WATCH|CONNECT"}[5m])) by (verb) / 1e+06
          record: apiserver:apiserver_request_latencies:avg_by_verb
    - name: scheduler.rules
      rules:
        - expr: |
            sum(up{job="kube-scheduler"} == 1)
          record: scheduler:up:sum
        - expr: |
            sum(scheduler_schedule_attempts_total{job="kube-scheduler"}) by (result)
          record: scheduler:scheduler_schedule_attempts:sum
        - expr: |
            sum(rate(scheduler_schedule_attempts_total{job="kube-scheduler"}[5m])) by (result)
          record: scheduler:scheduler_schedule_attempts:sum_rate
        - expr: |
            (sum(rate(scheduler_e2e_scheduling_latency_microseconds_sum{job="kube-scheduler"}[1h]))  / sum(rate(scheduler_e2e_scheduling_latency_microseconds_count{job="kube-scheduler"}[1h]))) /  1e+06
          record: scheduler:scheduler_e2e_scheduling_latency:avg
    - name: scheduler_histogram.rules
      rules:
        - expr: |
            histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_latency_microseconds_bucket{job="kube-scheduler"}[1h])) by (le) ) / 1e+06
          labels:
            quantile: "0.99"
          record: scheduler:scheduler_e2e_scheduling_latency:histogram_quantile
        - expr: |
            histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_latency_microseconds_bucket{job="kube-scheduler"}[1h])) by (le) ) / 1e+06
          labels:
            quantile: "0.9"
          record: scheduler:scheduler_e2e_scheduling_latency:histogram_quantile
        - expr: |
            histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_latency_microseconds_bucket{job="kube-scheduler"}[1h])) by (le) ) / 1e+06
          labels:
            quantile: "0.5"
          record: scheduler:scheduler_e2e_scheduling_latency:histogram_quantile
    - name: controller_manager.rules
      rules:
        - expr: |
            sum(up{job="kube-controller-manager"} == 1)
          record: controller_manager:up:sum
    - name: coredns.rules
      rules:
        - expr: |
            sum(up{job="coredns"} == 1)
          record: coredns:up:sum
        - expr: |
            sum(irate(coredns_cache_hits_total{job="coredns"}[5m]))
          record: coredns:coredns_cache_hits_total:sum_irate
        - expr: |
            sum(irate(coredns_cache_misses{job="coredns"}[5m]))
          record: coredns:coredns_cache_misses:sum_irate
        - expr: |
            sum(irate(coredns_dns_request_count_total{job="coredns"}[5m]))
          record: coredns:coredns_dns_request_count:sum_irate
        - expr: |
            sum(irate(coredns_dns_request_type_count_total{job="coredns"}[5m])) by (type)
          record: coredns:coredns_dns_request_type_count:sum_irate
        - expr: |
            sum(irate(coredns_dns_response_rcode_count_total{job="coredns"}[5m])) by (rcode)
          record: coredns:coredns_dns_response_rcode_count:sum_irate
        - expr: |
            sum(irate(coredns_panic_count_total{job="coredns"}[5m]))
          record: coredns:coredns_panic_count:sum_irate
        - expr: |
            sum(irate(coredns_proxy_request_count_total{job="coredns"}[5m]))
          record: coredns:coredns_proxy_request_count:sum_irate
        - expr: |
            sum(irate(coredns_dns_request_duration_seconds_sum{job="coredns"}[5m])) / sum(irate(coredns_dns_request_duration_seconds_count{job="coredns"}[5m]))
          record: coredns:coredns_dns_request_duration:avg
        - expr: |
            sum(irate(coredns_proxy_request_duration_seconds_sum{job="coredns"}[5m])) / sum(irate(coredns_proxy_request_duration_seconds_count{job="coredns"}[5m]))
          record: coredns:coredns_proxy_request_duration:avg
    - name: coredns_histogram.rules
      rules:
        - expr: |
            histogram_quantile(0.99, sum(irate(coredns_dns_request_duration_seconds_bucket{job="coredns"}[5m])) by (le))
          labels:
            quantile: "0.99"
          record: coredns:coredns_dns_request_duration:histogram_quantile
        - expr: |
            histogram_quantile(0.9, sum(irate(coredns_dns_request_duration_seconds_bucket{job="coredns"}[5m])) by (le))
          labels:
            quantile: "0.9"
          record: coredns:coredns_dns_request_duration:histogram_quantile
        - expr: |
            histogram_quantile(0.5, sum(irate(coredns_dns_request_duration_seconds_bucket{job="coredns"}[5m])) by (le))
          labels:
            quantile: "0.5"
          record: coredns:coredns_dns_request_duration:histogram_quantile
        - expr: |
            histogram_quantile(0.99, sum(irate(coredns_proxy_request_duration_seconds_bucket{job="coredns"}[5m])) by (le))
          labels:
            quantile: "0.99"
          record: coredns:coredns_proxy_request_duration:histogram_quantile
        - expr: |
            histogram_quantile(0.9, sum(irate(coredns_proxy_request_duration_seconds_bucket{job="coredns"}[5m])) by (le))
          labels:
            quantile: "0.9"
          record: coredns:coredns_proxy_request_duration:histogram_quantile
        - expr: |
            histogram_quantile(0.5, sum(irate(coredns_proxy_request_duration_seconds_bucket{job="coredns"}[5m])) by (le))
          labels:
            quantile: "0.5"
          record: coredns:coredns_proxy_request_duration:histogram_quantile
    - name: prometheus.rules
      rules:
        - expr: |
            sum(up{job=~"prometheus-k8s.*"} == 1)
          record: prometheus:up:sum
        - expr: |
            sum(rate(prometheus_tsdb_head_samples_appended_total{job=~"prometheus-k8s.*"} [5m])) by (job, pod)
          record: prometheus:prometheus_tsdb_head_samples_appended:sum_rate
